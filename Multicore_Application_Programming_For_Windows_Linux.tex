\documentclass[12pt,a4paper]{report}
%\usepackage{charter}
\usepackage[latin1]{inputenc}
\usepackage[left=1.50cm, right=1.50cm, top=1.20cm]{geometry}
\renewcommand{\baselinestretch}{1.5}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\usepackage{float}
\usepackage{textcomp}
\lstdefinestyle{customc}{
	belowcaptionskip=1\baselineskip,
	aboveskip={1.2\baselineskip},
	breaklines=true,
	frame=lines,
	numbers=left,
	xleftmargin=\parindent,
	language=C++,
	showstringspaces=false,
	basicstyle=\sffamily,%,\ttfamily,
	keywordstyle=\bfseries\color{green!40!black},
	commentstyle=\itshape\color{purple!40!black},
	identifierstyle=\color{blue},
	stringstyle=\color{orange},
	breaklines=true,
	postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}
}

\lstdefinestyle{customasm}{
	belowcaptionskip=1\baselineskip,
	frame=L,
	xleftmargin=\parindent,
	language=[x86masm]Assembler,
	basicstyle=\footnotesize\ttfamily,
	commentstyle=\itshape\color{purple!40!black},
}
\lstset{escapechar=@,style=customc}

% Title Page
\title{Reading Notes: Multicore Application Programming: Windows, Linux and Oracle Solaris}


\begin{document}
\maketitle

\chapter{Hardware, Processes and Threads}
\section{Cache}
 When the data is fetched from memory, it is fetched together with the surrounding bytes as a cache line, as shown in below. 
 \begin{center}
 	\includegraphics{multicore_cache_line_from_memory.jpg}
 \end{center}
 Depending on the processor in a system, a cache line might be as small as 16 bytes, or it could be as large as 128 (or more) bytes. A typical value for cache line size is 64 bytes.
 \par
 Cache lines are always aligned, so a 64-byte cache line will start at an address that is a multiple of 64. This design decision simplifies the system because it enables the system to be optimized to pass around aligned data of this size; the alternative is a more complex memory interface that would have to handle chunks of memory of different sizes and differently aligned start addresses.
 \par
 When a line of data is fetched from memory, it is stored in a cache. Caches improve performance because the processor is very likely to either reuse the data or access data stored on the same cache line. There are usually caches for \textbf{instructions} and caches for \textbf{data}. There may also be multiple levels of cache.
 \par
 The reason for having multiple levels of cache is that the \textbf{larger} the size of the cache, the \textbf{longer} it takes to determine whether an item of data is held in that cache.
 \par
 A processor might have a small first-level cache that it can access within a few clock cycles and then a second-level cache that is much larger but takes tens of cycles to access. Both of these are significantly faster than memory, which might take hundreds of cycles to access. The time it takes to fetch an item of data from memory or from a level of cache is referred to as its \textbf{latency}.
 \par
 Caches have two very obvious characteristics: the \textbf{size} of the \textbf{cache lines} and the \textbf{size} of the \textbf{cache}. The number of lines in a cache can be calculated by \textbf{dividing} one by the other. For example, a 4KB cache that has a cache line size of 64 bytes will hold 64 lines.
 \par
 Caches have one characteristic that is the associativity. In a simple cache, each cache line in memory would map to exactly one position in the cache; this is called a \textbf{direct mapped cache}. If we take the simple 4KB cache outlined earlier, then the cache line located at every 4KB interval in memory would map to the same line in the cache.
\par
Obviously, a program that accessed memory in 4KB strides would end up just using a single entry in the cache and could suffer from poor performance if it needed to simultaneously use multiple cache lines.
\par
The way around this problem is to increase the \textbf{associativity} of the cache---that is, make it possible for a single cache line to map into more positions in the cache and therefore reduce the possibility of there being a conflict in the cache.
\par
In a 2--way associative cache, each cache line can map into one of two locations. The location is chosen according to some \textbf{replacement policy} that could be random replacement, or it could depend on which of the two locations contains the oldest data (least recently used replacement).
\par
Doubling the number of potential locations for each cache line means that the interval between lines in memory that map onto the same cache line is halved, but overall this change will result in more effective utilization of the cache and a reduction in the number of cache misses.
\begin{center}
	\includegraphics{multicore_2_way_cache_line.jpg}
\end{center}
\section{Virtual Memory}
The critical step in using virtual memory is the translation of a virtual address, as used by an application, into a physical address, as used by the processor, to fetch the data from memory. This step is achieved using a part of the processor called the translation look--aside buffer (\textbf{TLB}). Typically, there will be one \textbf{TLB} for translating the address of \textbf{instructions} (the instruction TLB or \textbf{ITLB}) and a second TLB for translating the address of \textbf{data} (the data TLB, or \textbf{DTLB}).
\par
Each \textbf{TLB} is a list of the virtual address range and corresponding physical address range of each page in memory. So when a processor needs to translate a virtual address to a physical address, it first splits the address into a virtual page (the high--order bits) and an offset from the start of that page (the low--order bits). It then looks up the address of this virtual page in the list of translations held in the TLB. It gets the physical address of the page and adds the offset to this to get the address of the data in physical memory. It can then use this to fetch the data.
\begin{center}
	\includegraphics{multicore_TLB.jpg}
\end{center}
A \textbf{TLB} can hold only a limited set of translations. So, sometimes a processor will need to find a physical address, but the translation does not reside in the TLB. In these cases, the translation is fetched from an in--memory data structure called a \textbf{page table}, and this structure can hold many more virtual to physical mappings. 
\par
When a translation does not reside in the \textbf{TLB}, it is referred to as a \textbf{TLB miss}, and TLB misses have an impact on performance. The magnitude of the performance impact depends on whether the hardware fetches the TLB entry from the page table or whether this task is managed by software; most current processors handle this in hardware. It is also possible to have a\textbf{ page table miss}, although this event is very rare for most applications. The page table is managed by software, so this typically is an expensive or slow event.
\chapter{Synchronization and Sharing}
\section{Atomic Operations}
On x86 processors, the xadd instruction can be combined with the lock prefix to produce an atomic add, or the inc instruction can be combined with the lock prefix to produce an atomic increment.
\begin{lstlisting}
int atomic_add_int( volatile int *address, int value )
{
	asm volatile( "lock xadd %0,%1":
					"+r"(value):
					"+m"(*address):
					"memory" 
				);
	return value;
}
int atomic_inc_int( int *address )
{
	asm volatile ( "lock inc %0": :
					"+m"(*address):
					"memory" 
				);
	return (*address);
}
\end{lstlisting}
\begin{itemize}
	\item The keyword \textbf{asm} identifies the following text as an assembly language statement that will be inlined directly into the code. 
	\item he keyword \textbf{volatile} tells the compiler not to move the statement from where it has been placed, because such movement could cause a difference to the statement's semantics.
	\item The assembly language code is enclosed in the parentheses. There are multiple parts to the code. The first item in the parentheses, surrounded by quotes, is the instruction. The instruction uses virtual registers \%0 and \%1 as parameters. The compiler will allocate real registers later and will be responsible for populating these registers with the appropriate values.
	\item After the assembly language instruction, there are up to three colon-delimited lists. 
	\begin{itemize}
		\item The first list is of output variables and whether these are accesses to registers or memory. In the example, the expression ``+r''(value) means that the output parameter is a register that should be used to update the variable value. The plus sign means that the register will be both read and written by the instruction.
		\item 	The second list contains the input values and where these are located. Both routines take the pointer address as an input value, and the expression ``+m''(*address) indicates that this pointer is used as a memory access. The plus sign indicates that the instruction will both read and write the location in memory.
		\item The third list is the ``clobber'' list indicating what the instruction will modify. In both instances, the instruction will modify memory.
	\end{itemize}
	\item The virtual registers are numbered from the input registers, so register \%0 is assigned the value of the variable address. The output registers are the next set of virtual registers, so the variable value gets assigned to register \%1.
\end{itemize}
\subsubsection*{CAS Operation}
The CAS instruction can also be used in situations where atomic modification of an unusual variable type is required.
\begin{lstlisting}
void atomic_add_float( volatile float * variable, float increment )
{
	union
	{
		int asint;
		float asfp;
	} oldvalue, newvalue;
	
	do
	{
		oldvalue.asfp = *variable;                  // Line 11
		newvalue.asfp = oldvalue.asfp + increment;  // Line 12
	}
	while ( CAS( variable, oldvalue.asint, newvalue.asint ) != oldvalue.asint );
}
\end{lstlisting}
\begin{itemize}
	\item The code reads the value of the \textit{variable} to be modified and then prepares the modified version of this variable. 
	\item The \textit{variable} is read only once, at line 11, and then held in the local variable \textit{oldvalue}. This is to avoid a \textbf{data race} where the value changes between the first read of the variable and its use as one operand in the addition, at line 12. The race is subtle.
	\begin{itemize}
		\item Suppose that the first time the \textit{variable} is read, at line 11, it has the value 20; this is recorded as the original value of the variable. At this point, another thread changes the value of the variable, so the second time it is read, at line 12, it has the value 21. The value 21 is incremented to 22, and this will become the new value of the variable if the operation is successful.
		\item Suppose now that the other thread has returned the value of the variable to 20 by the time the CAS operation is tried. Since the variable contains 20 and this is the expected value, the CAS operation will store the value 22 into the variable, rather than the correct value of 21.
	\end{itemize} 
\end{itemize}
We can explore this problem of reloading values with another example.
\begin{lstlisting}
void addelement( element_t ** head, int value )
{
	element_t * element = (element_t*)malloc(sizeof(element_t));
	element->value = value;
	while (element!=0)
	{
		element->next = *head;
		if ( CAS( head, element->next, element ) == element->next)
	{
		element = 0;
	}
}
\end{lstlisting}
The code creates a new \textit{element} and stores the appropriate value in this. 
\begin{itemize}
	\item The loop keeps attempting to add this new element to the front of the list until it succeeds. 
	\item Each iteration of the loop sets the next field of the new element to be the next element in the list and then attempts to atomically compare and swap the head of the list with a pointer to the new element. 
	\item If the compare and swap succeeded, the return value will be the pointer to the element that used to be the top of the list. If this happens, the code has succeeded, and the loop can exit.
\end{itemize}
The problem with this code is that the \textbf{CAS()} function call causes the compiler to reload the value of \textbf{element\textrightarrow next}. 
\begin{itemize}
	\item There is a short window of opportunity for another thread to take the top element from the list between the \textbf{CAS()} function call and the load of \textbf{element\textrightarrow next}. 
	\item If this other thread takes the element off the list and modifies the value of \textbf{element\textrightarrow next}, then the compare and swap will not match with the new value of \textbf{element\textrightarrow next}, and the loop will assume that it failed to add the element to the queue, even though it actually succeeded. So, the code will attempt to add the element a second time, causing an error.
\end{itemize}
The solution is to hold the original value of *head in a local variable and use this in the comparison to determine whether the compare and swap was successful. 
\par
\textbf{Notice:}
cppreference.com give the following example:
\begin{lstlisting}
#include <atomic>
template<typename T>
struct node
{
	T data;
	node* next;
	node(const T& data) : data(data), next(nullptr) {}
};

template<typename T>
class stack
{
	std::atomic<node<T>*> head;
public:
void push(const T& data)
{
	node<T>* new_node = new node<T>(data);

	// put the current value of head into new_node->next
	new_node->next = head.load(std::memory_order_relaxed);

	// now make new_node the new head, but if the head
	// is no longer what's stored in new_node->next
	// (some other thread must have inserted a node just now)
	// then put that new head into new_node->next and try again
	while(!head.compare_exchange_weak(new_node->next, new_node,
	std::memory_order_release,
	std::memory_order_relaxed))
	; // the body of the loop is empty

// Note: the above use is not thread-safe in at least 
// GCC prior to 4.8.3 (bug 60272), clang prior to 2014-05-05 (bug 18899)
// MSVC prior to 2014-03-17 (bug 819819). The following is a workaround:
//      node<T>* old_head = head.load(std::memory_order_relaxed);
//      do {
//          new_node->next = old_head;
//       } while(!head.compare_exchange_weak(old_head, new_node,
//                               std::memory_order_release,
//                               std::memory_order_relaxed));
}
};
int main()
{
	stack<int> s;
	s.push(1);
	s.push(2);
	s.push(3);
}
\end{lstlisting}
\subsection{Memory Order}
When a thread running on a multithread system performs a memory operation, that operation may or may not become visible to the rest of the system in the order in which it occurred.
\par
For example, an application might perform two store operations. On some processors, the second store operation could become visible to other processors before the first store operation becomes visible. This is called \textbf{weak memory ordering}.
\par
The kinds of memory barriers available are defined by the architecture. The x86 architecture defines the following:
\begin{itemize}
	\item \textbf{mfence}. Ensures that all previous loads and stores are visible to the system before any future loads and stores become visible.
	\item \textbf{sfence}. Ensures that all previous stores are visible to the system before any future stores become visible.
	\item \textbf{lfence}. Ensures that all previous loads are visible to the system before any future loads become visible
\end{itemize}
\subsection{Reordering by Compiler}
It is often thought that the \textbf{volatile} keyword provides a safety net that stops the compiler from optimizing code. Unfortunately, this is not true.
\par
The \textbf{volatile} keyword determines that the compiler needs to reload data from memory when it needs to use it and should store it back to memory as soon as it is modified. It does not stop the compiler from performing optimizations around the data nor does it form any kind of protection against data races.
\par
However, the \textbf{volatile} keyword is necessary to avoid the compiler optimizing away accesses to a variable.
\begin{lstlisting}
volatile int start;

void waitforstart()
{
	while( start == 0 ) {}
}
\end{lstlisting}
The variable \textit{start} is declared as \textbf{volatile}. If this were not the case, then the compiler would determine that the loop was either not entered or entered and infinite.
\begin{lstlisting}
volatile int count;

int* getnextelement()
{
	int element = 0;
	while( element == 0 )
	{
		if (count>0)
		{
			element = getElementFromList();
		}
	}
	return element;
}
\end{lstlisting}
In this case, the code contains a function call. Function calls, generally, cause the compiler to reload global data. However, if the variable \textit{count} is not declared to be \textbf{volatile}, it is still possible for the compiler to generate an infinite loop. 
\begin{itemize}
	\item The reason for this is that if \textit{count} does equal zero and it is not a \textbf{volatile} variable, then the call to \textit{getElementFromList()} will not be made, and the variable will remain zero. An optimizing compiler may identify this situation and replace the \textbf{else} branch of the \textbf{if} statement with an infinite loop.
\end{itemize}
\begin{lstlisting}
#include <stdio.h>
#include <pthread.h>

volatile int start[4];
volatile int done[4];

void * work( void* param )
{
	int id = (int)param;
	while ( start[id] == 0 ) {}
	printf( "Thread %i started\n", id );
	double total=0;
	for ( int i=0; i<100000000; i++ ) { total += i; }
	printf( "Thread %i done\n", id );
	done[id] = 1;
}
int main()
{
	pthread_t thread[4];
	for ( int i=0; i<4; i++ )
	{
		pthread_create( &thread[i], 0, work, (void*)i );
		done[i]=0;
	}
	for ( int i=0; i<4; i++ )
	{
		start[i] = 1;
	}
	for ( int i=0; i<4; i++ )
	{
		while( done[i] == 0 ){}
	}
	for ( int i=0; i<4; i++ )
	{
		pthread_join( thread[i], 0 );
	}
}
\end{lstlisting}
In this case,  the compiler can merge two loops, resulting in a change of behavior.
\begin{itemize}
	\item The code creates four threads. Each thread runs the same routine that prints out a statement that the thread is ready and then waits on the \textbf{volatile} variable \textit{start}.
	\item When the thread is started, it prints out a message before completing a short amount of work, printing a message indicating that it has completed its work and exiting.
	\item The main code creates the threads and, once all the threads are created, signals to the threads that they can start working. Once this has been done, it waits for all the threads to complete their work, before calling \textit{pthread\_join()} on each of the threads.
\end{itemize}
When compiled \textbf{without optimization}, all the threads proceed at the same time, so the output from the application shows all the threads starting and then all the threads completing. 
\par
When compiled \textbf{with optimizatio}n, the threads are \textbf{serialized}, so each thread prints the message that it has started followed by the message that it has completed.
\begin{lstlisting}
$ cc loop_merge.c
$ ./a.out
	Thread 1 started
	Thread 2 started
	Thread 0 started
	Thread 3 started
	Thread 1 done
	Thread 0 done
	Thread 2 done
	Thread 3 done
$ cc -O loop_merge.c
$ ./a.out
	Thread 0 started
	Thread 0 done
	Thread 1 started
	Thread 1 done
	Thread 2 started
	Thread 2 done
	Thread 3 started
	Thread 3 done
\end{lstlisting}
With optimization, the compiler has merged the code in the two loops that set the start variable and read the end variable. This produces code similar to that shown in the following.
\begin{lstlisting}
int main()
{
	pthread_t thread[4];
	for( int i=0; i<4; i++ )
	{
		pthread_create( &thread[i], 0, work, (void*)i );
		done[i] = 0;
	}
	for ( int i=0; i<4; i++ )
	{
		start[i] = 1;
		while( done[i] == 0 ){}
	}
	for ( int i=0; i<4;i++ )
	{
		pthread_join( thread[i], 0 );
	}
}
\end{lstlisting}
The two loops contain no function calls, and the compiler considers the accesses to the volatile variables to be without side effects. Hence, the compiler considers it safe to merge the two loops.
\par
To correct this situation, we need to modify the code so that the compiler is unable to merge the two loops.
\begin{itemize}
	\item The easiest way to do this is to place a\textbf{ function call} either in one of the loops or between the two loops.
	\item An alternative approach would be to separate the two loops with serial code that is unable to be moved because of some dependency or potential aliasing issue.
\end{itemize}
However, both of these approaches will add some unnecessary instructions into the code, and both approaches run the risk that a ``smart'' compiler might identify the instructions as unnecessary and remove them.
\par
The good slution is to use a \textbf{gcc asm(:::memory)} construct that cause the compiler to correctly order the loops. This statement stops the loops from merging and adds no additional instructions into the code.
\begin{lstlisting}

int main()
{
	pthread_t thread[4];
	for ( int i=0; i<4; i++ )
	{
		pthread_create( &thread[i], 0, work, (void*)i );
		done[i] = 0;
	}
	for ( int i=0; i<4; i++ )
	{
		start[i] = 1;
	}

	asm volatile( "": : : "memory" );
	for ( int i=0; i<4; i++ )
	{
		while( done[i] == 0 ){}
	}
	for ( int i=0; i<4;i++ )
	{
		pthread_join( thread[i], 0 );
	}
}

\end{lstlisting}
Windows provides intrinsics for this purpose. The functions \textbf{\_ReadBarrier()}, \textbf{\_WriteBarrier()}, and \textbf{\_ReadWriteBarrier()} are defined in \textbf{intrin.h}. These intrinsics tell the compiler not to reorder the operations.
\begin{itemize}
	\item A \textbf{\_ReadBarrier()} call ensures that all reads will have completed at that point
	\item A \textbf{\_WriteBarrier()} call ensures that all writes have completed
	\item These instructions only enforce the compiler ordering and do not generate any instructions.
\end{itemize}
\paragraph*{Volatile Variables}
Using the \textbf{volatile} keyword is necessary to avoid undesirable \textbf{caching}, in registers, of the values held in memory locations. However, the keyword also stops desirable caching of the variable, so any use of the variable can be \textbf{expensive}.
\par
However, it should be observed that use of compiler memory barriers can be \textbf{low cost} and a more accurate way of ensuring that variables are stored back to memory and reloaded from memory at the desired point in the code.
\begin{lstlisting}
extern int start;

void waitforstart()
{
	while(start==0) { asm volatile( "": : : "memory" ); }
}

\end{lstlisting}
This case avoid having to declare the variable \textit{start} as \textbf{volatile} and instead uses a \textbf{compiler barrier} to ensure that the value is reloaded from memory.
\section{Lock--Free Algorithms}
\begin{lstlisting}
volatile int priority = 0;
volatile int counter = 0;
volatile int waiting[2];

void increment( int id )
{
	waiting[id] = 1;

	while( waiting[1-id] == 1 )
	{
		if ( priority != id )
		{
			waiting[id] = 0;
			while ( priority != id ){}
			waiting[id] = 1;
		}
	}
	/* Critical section */
	counter++;
	/* Exit critical section */
	priority = 1-id;
	waiting[id] = 0;
}
\end{lstlisting}
\subsection{Dekker's Algorithm}
The algorithm works because each thread signals that it is waiting to get into the critical section. If the other thread has not signaled that it is waiting for or has already entered the critical section, then the current thread can enter it. If both threads are waiting, then one of the threads gets priority, and the other thread waits until it gets \textit{priority}.
\\
The problem with the code is one of memory ordering.
\begin{itemize}
	\item The two threads can simultaneously indicate that they are waiting to enter the critical section by storing to their index in the waiting array.
	\item In the very next cycle (i.e. when both threads running to \textbf{line 9}), they load the other thread's waiting status. Since the other thread has only just issued the store, the \textbf{store} has not yet made its way through the pipeline to be visible to the rest of the system.
	\item So, the load instruction picks up a zero, indicating that the other thread is not waiting for the critical section. 
	\item Both threads fail to see the other thread waiting, and both threads enter the critical region.
\end{itemize}
The way to fix this is to put memory barriers into the code.
\begin{itemize}
	\item The memory barriers ensure that previous memory operations have completed before the next memory operation issues.
	\item In this case, we want to ensure that the \textbf{store} to indicate a thread is waiting is completed \textbf{before} the \textbf{load} to check whether the other thread is also waiting.
	\item Consider the sequence of operations. Both threads hit the \textbf{store} at the same time; both threads now wait until the \textbf{stores} are \textbf{visible} to the other processor before issuing their load. So, both threads will get the data that was stored by the other thread.
\end{itemize}
On \textbf{x86} processors, it is necessary to use an \textbf{mfence} operation that ensures that all previous memory operations complete before the next memory operation is issued.
\begin{lstlisting}
#include <stdio.h>
#include <pthread.h>

volatile int priority = 0;
volatile int counter = 0;
volatile int waiting[2];

void increment( int i )
{
	waiting[i] = 1;

	#ifdef __sparc
		asm( "membar #StoreLoad": : : "memory" );
	#else
		asm( "mfence": : : "memory" );
	#endif
	
	while( waiting[1-i] == 1 )
	{
		if ( priority != i )
		{
			waiting[i] = 0;
			while ( priority != i ){}
			waiting[i] = 1;
			#ifdef __sparc
				asm( "membar #StoreLoad": : : "memory" );
			#else
				asm( "mfence": : :"memory" );
			#endif
		}
	}

	counter++;
	priority = 1-i;
	waiting[i] = 0;
}
\end{lstlisting}
\subsection{Producer--Consumer with a Circular Buffer}
Consider the case where there is a producer-consumer pair of threads that communicate through a circular buffer. It is possible to write code that does not require locks or atomic operations to handle this situation.
\begin{lstlisting}
#include <stdio.h>
#include <pthread.h>
#include <stdlib.h>

volatile int volatile buffer[16];
volatile int addhere;
volatile int removehere;

void clearbuffer()
{
	addhere = 0;
	removehere = 0;
	for( int i=0; i<16; i++ ) { buffer[i] = 0; }
}

int next( int current )
{
	return ( current+1 ) & 15;
}

void addtobuffer( int value )
{
	while( next(addhere) == removehere ) {} // Spin waiting for room
	if ( buffer[addhere] != 0 )
	{ printf( "Circular buffer error\n" ); exit(1); }
	buffer[addhere] = value;                // Add item to buffer
	addhere = next(addhere);                // Move to next entry
}

int removefrombuffer()
{
	int value;
	while( ( value = buffer[removehere] ) == 0 ){} // Spin until
	// something in buffer
	buffer[removehere] = 0;                        // Zero out element
	removehere = next(removehere);                 // Move to next entry
	return value;
}
\end{lstlisting}
There are actually two implicit constraints that ensure that the code works. One constraint is that stores and loads are themselves atomic. The other constraint is that stores do not become reordered.
\\
The requirement for ordering stores comes from the code that removes elements from the array.
\begin{itemize}
	\item The location containing the element to be removed is zeroed out, then the pointer to the end element to be removed is advanced to the next location in the array.
	\item If these actions became visible to the producer thread in the \textbf{wrong order}, the producer thread would see that the end pointer had been advanced. This would allow it to enter the code that adds a new element.
	\item However, the first test this code performs is to check that the new location is really empty. If the store of zero to the released array position was delayed, this location would still contain the old value, and the code would exit with an error. The check validates that the code is behaving correctly.
\end{itemize}
Several characteristics of the algorithm enable it to work correctly.
\begin{itemize}
	\item There are 2 pointers that point to locations in the array. Only one thread is responsible for updating each of these variables. The other thread only reads the variable.
	\item As long as the \textbf{reading thread} sees the updates to memory in the correct order and each variable is updated \textbf{atomically}, the thread will continue to wait until the data is ready and only then read that data.
	\item The updates of the pointers into the array are carried out by a \textbf{function call}. The function call acts as a \textbf{compiler memory barrier} and forces the compiler to store variables back to memory. Otherwise, there would be the risk that the compiler might reorder the store operations.
\end{itemize}
\subsection{Scaling the Producer-Consumer to Multiple Threads}
The simplest way to share the circular buffer between threads would be to use some kind of mutual exclusion that would ensure that only a single thread at a time could modify the structure. 
\begin{lstlisting}
#include <stdio.h>
#include <pthread.h>

volatile int volatile buffer[16];
volatile int addhere;
volatile int removehere;
volatile int lock = 0;

void lock_spinlock( volatile int* lock )
{
	while ( CAS( lock, 0, 1 ) != 0 ) {}
	acquire_memory_barrier();
}

void free_spinlock( volatile int *lock )
{
	release_memory_barrier();
	*lock = 0;
}
void clearbuffer()
{
	addhere = 0;
	removehere = 0;
	for( int i=0; i<16; i++ ) { buffer[i] = 0; }
}

int next( int current )
{
	return ( current + 1 ) & 15;
}
void addtobuffer( int value )
{
	lock_spinlock( &lock );
	while( buffer[ next(addhere) ] != 0 ) {}
	buffer[addhere] = value;
	addhere = next(addhere);
	free_spinlock( &lock );
}

int removefrombuffer()
{
	int value;
	lock_spinlock( &lock );
	while( ( value = buffer[removehere] ) == 0 ){}
	buffer[removehere] = 0;
	removehere = next(removehere);
	free_spinlock( &lock );
	return value;
}
\end{lstlisting}
The code now contains a deadlock. Imagine that the circular buffer is empty and a consumer thread acquires the lock and starts waiting for something to appear in the buffer. A producer thread eventually arrives but now has to acquire the lock before it can add anything into the buffer. Both threads end up spinning, waiting for an event that can never happen.
\\
There are 2 solutions to this particular instance of the problem.
\\
One solution is to provide two locks---one for threads waiting to add elements to the buffer and the second for threads waiting to remove an item from the list. This solution works in this situation because it reduces the problem down to the one described earlier where there are only two threads present in the system.
\begin{itemize}
	\item 	One lock ensures that only one producer thread can access the circular buffer at a time. 
	\item The other lock ensures that only one consumer thread can access the circular buffer at a time.
	\item The other producer and consumer threads cannot interfere and cause correctness problems.
\end{itemize}
Another solution uses only one single lock. One way of doing this is to place the critical section inside another loop, which repeats the loop until the critical section is successfully executed. This requires modification to the \textbf{addtobuffer()} and \textbf{removefrombuffer()} routines so that they no longer loop inside the critical section and instead quickly return success or failure.
\begin{lstlisting}
void addtobuffer( int value )
{
	int success = 0;
	while( !success )
	{
		lock_spinlock( &lock );
		if( buffer[ next(addhere) ] == 0 )
		{
			buffer[addhere] = value;
			addhere = next(addhere);
			success = 1;
		}
		free_spinlock( &lock );
	}
}

int removefrombuffer()
{
	int value;
	int success = 0;
	while ( !success )
	{
		lock_spinlock( &lock );
		if ( ( value = buffer[removehere] ) != 0 )
		{
			buffer[removehere] = 0;
			removehere = next(removehere);
			success = 1;
		}
		free_spinlock( &lock );
	}
	return value;
}
\end{lstlisting}
The code uses a variable \textit{success} to determine whether the critical region was successful. Although this change results in the desired behavior for the code, it is not the best code to run on the system. The problem with the code is that while threads are unable to add or remove items from the queue, the spinlock is constantly being acquired and released.
\begin{itemize}
\item This results in significant traffic between the cores invalidating and fetching the cache line containing the lock variable.
\item Both the acquisition and release of the variable lock result in a store operation, which causes all the other caches to invalidate the cache line containing the lock.
\item When the next thread acquires the spinlock, it has to fetch it from the cache of the processor that last updated the lock.
\item This activity causes the cache line containing the variable lock to be constantly being passed between the caches of different virtual CPUs and may have an impact on the performance of the system.
\end{itemize}
One way of improvement is to add a test success in accessing the buffer before acquiring the lock.
\begin{lstlisting}
void addtobuffer( int value )
{
	int success = 0;
	while ( !success )
	{
		if ( buffer[ next(addhere) ] == 0 ) // Wait for an empty space
		{
			lock_spinlock( &lock );
			if( buffer[ next(addhere) ] == 0 )
			{
				buffer[addhere] = value;
				addhere = next(addhere);
				success = 1;
			}
			free_spinlock( &lock );
		}
	}
}

int removefrombuffer()
{
	int value;
	int success = 0;
	while ( !success )
	{
		if ( buffer[removehere] != 0 ) // Wait for an item to be added
		{
			lock_spinlock( &lock );
			if ( ( value = buffer[removehere] ) != 0 )
			{
				buffer[removehere] = 0;
				removehere = next(removehere);
				success = 1;
			}
			free_spinlock( &lock );
		}
	}
	return value;
}
\end{lstlisting}
\begin{itemize}
		\item This test for success is to load the next element in the buffer array and see whether it is zero. 
		\item The advantage of using \textbf{load} instructions is that the cache line fetched by a load remains resident in cache until it is invalidated by a store operation.
		\item In practical terms, each thread will spin on the appropriate variable waiting for it to be modified. This causes \textbf{no invalidation} of the values held in other caches until the variable is actually updated.
		\item Consequently, there is little risk of there being a performance impact from this scheme.
\end{itemize}
The problem with this code is that every time a new item is added to the circular buffer or every time a space becomes free, all the waiting threads recognize this and attempt to acquire the lock, even though only one thread can actually succeed.
\\
This is an example of the \textbf{thundering herd problem} where a number of threads are waiting for a condition to become true and \textbf{only one thread can successfully proceed}, so all the other threads end up using unnecessary resources. This problem can be resolved by \textbf{ordering} the list of threads such that only one thread is allowed to proceed.
\\
However, the problem is worse than this. All the threads that identify the opportunity to access the circular buffer will enter the \textbf{if} statement and can exit only after they have acquired and released the spinlock. So, these threads will end up spinning on the spinlock, which was not the intention of the code.
\\
To remove this problem, we should change the code so that instead of spinning on the spinlock, the threads try to acquire it, and if they do not, then they should return to the outer loop and wait for the next opportunity to access the circular buffer.
\begin{lstlisting}
int try_spinlock( volatile int* lock )
{
	if ( CAS( lock, 0, 1 ) == 1 ) { return 0; }
	else
	{
		acquire_memory_barrier();
		return 1;
	}
}

void addtobuffer( int value )
{
	int success = 0;
	while ( !success )
	{
		if ( buffer[ next(addhere) ] == 0 )
		{
			if (try_spinlock( &lock ) )
			{
				if ( buffer[ next(addhere) ] == 0 )
				{
					buffer[addhere] = value;
					addhere = next(addhere);
					success = 1;
				}
				free_spinlock( &lock );
			}
		}
	}
}

int removefrombuffer()
{
	int value;
	int success = 0;
	while ( !success )
	{
		if ( buffer[removehere] != 0 )
		{
			if ( try_spinlock( &lock ) )
			{
				if ( ( value = buffer[removehere] ) !=0 )
				{
					buffer[removehere] = 0;
					removehere = next(removehere);
					success = 1;
				}
				free_spinlock( &lock );
			}
		}
	}
	return value;
}
\end{lstlisting}
It introduces a function called \textbf{try\_spinlock()} that will either \textbf{acquire} the \textbf{spinlock} and return \textbf{true} or \textbf{fail} to acquire the lock and return \textbf{false}. With this modification, 
\begin{itemize}
\item the threads spin on the variable, indicating the state of the circular buffer. 
\item This variable is shared so it does not produce much memory traffic. 
\item If the state changes, the threads attempt to get the spinlock.
\item Only one thread will succeed, and this thread gets to access the buffer while the other threads go back to spinning on the shared variable.
\end{itemize}
With this change, the \textbf{spinlock} has ceased to be used as a spinlock since the threads spin before attempting to acquire the lock.
\\
We can make further improvement to reduce the number of CAS operations, we could spins until the lock has been \textbf{acquired} by the calling thread.
\begin{lstlisting}
void lock_spinlock( volatile int* lock )
{
	int acquired = 0;
	while ( !acquired )
	{
		if ( ( *lock == 0 ) && ( CAS( lock, 0, 1 ) == 0) )
		{
			acquired = 1;
		}
	}
	acquire_memory_barrier();
}

void addtobuffer( int value )
{
	int success = 0;
	while ( !success )
	{
		if ( buffer[ next(addhere) ] == 0 )
		{
			lock_spinlock( &lock );
			if ( buffer[ next(addhere) ] == 0 )
			{
				buffer[addhere] = value;
				addhere = next(addhere);
				success = 1;
			}
			free_spinlock( &lock );

		}
	}
}

int removefrombuffer()
{
	int value;
	int success = 0;
	while ( !success )
	{
		if ( buffer[removehere] != 0 )
		{
			lock_spinlock( &lock );
			if ( ( value = buffer[removehere] ) !=0 )
			{
				buffer[removehere] = 0;
				removehere = next(removehere);
				success = 1;
			}
			free_spinlock( &lock );
		}
	}
	return value;
}
\end{lstlisting}
In each iteration, the loop tests whether the lock is available. If the lock is available, the code attempts to acquire the lock atomically. If successful, the code \textbf{exits} the loop having acquired the spinlock. If unsuccessful, the loop continues to spin.

\subsection{Modifying the Producer-Consumer Code to Use Atomics}
\begin{lstlisting}
void addtobuffer( int value )
{
	int success = 0;
	while ( !success )
	{
		if ( buffer[ next(addhere) ] == 0 )
		{
			if ( CAS( &buffer[ next(addhere) ], 0, value ) == 0 )
			{
				addhere = next(addhere);
				success = 1;
			}
		}
	}
}

int removefrombuffer()
{
	int value;
	int success = 0;
	while ( !success )
	{
		if ( ( value = buffer[removehere] ) != 0 )
		{
			if ( CAS( &buffer[removehere], value, 0 ) == value )
			{
				removehere = next(removehere);
				success = 1;
			}
		}
	}
	return value;
}

\end{lstlisting}
It appears that this should work. Only one thread at a time can successfully add or remove an element. Only that thread will alter the pointer so that it points to the next entry.However, this is not the case.
\\
It is critical to realize that although instructions will execute in the expected order, the gaps between the execution of adjacent instructions are random. A pair of instructions could be executed on the same cycle or with a separation of only a few cycles.
\\
However, a thread may be \textbf{context switched off} the virtual CPU between the two instructions, and this would cause a gap of thousands of cycles to occur between the two operations.
\\
Consider the situation shown in following figure where multiple producer and consumer threads exist.
\begin{center}
	\includegraphics{multicore_getfile.jpg}
\end{center}
\begin{itemize}
\item  At \textbf{step A}, two producer threads are attempting to add an item into the circular buffer. They both reach the \textbf{CAS} instruction at nearly the same time, but one of the threads gets context switched off the CPU at that very moment.
\item At \textbf{step B}, the other thread successfully enters its value into the circular list and is just about to move the \textbf{addhere} pointer onto the next element when it too is context switched off the virtual CPU.
\item At\textbf{ step C}, while the first thread is off-processor, one of the consumer threads come along and removes the recently inserted element. At that point, the first thread is switched back onto the CPU, and it sees that the slot it was planning to use is now empty. It atomically inserts its item of data into the circular buffer and then increments the pointer to the next place to insert an element.
\item This is rapidly followed by the second producer thread being brought back onto a virtual CPU. It completes the operation it was performing when it was context switched off the virtual CPU, and it too increments the addhere pointer to the next place to insert an element. This causes the addhere pointer to skip a location, but the removehere pointer, which indicates where to remove elements from, is now pointing at the skipped location, as shown in step D.
\end{itemize}
The skipped location is empty, so the consumer threads are left waiting for an element to be inserted there. This cannot happen until the producer threads have filled up the entire circular buffer.
\par
However, before the producer threads can fill the entire buffer, they hit a filled element in the buffer and cannot make progress until this element has been removed, which will never happen because of the stalled consumer threads. (as shown in \textbf{step E})
\par
Consequently, the application deadlocks with both the producer and consumer threads unable to make forward progress.
\subsection{ABA Problem}
This example is an instance of a general problem called the \textbf{ABA problem}. The ABA problem is the situation where a thread is context \textbf{switched off} the virtual CPU when the program is in \textbf{state A}. While it is off--CPU, the system changes into a second\textbf{ state, B}, before returning to \textbf{state A}. This state is different from the original state but \textbf{looks identical} to the thread that is now switched back onto a virtual CPU. When the first thread returns to the CPU, it continues to \textbf{act as if the program state had not changed}, and this causes an error.
\par
In the circular buffer problem, the first thread is taken off the virtual CPU while believing it has a pointer to a free slot. When it returns to CPU, it has a pointer to what it believes is the free slot, but it has no indication that the state of the rest of the system has changed.
\par
A general solution to the ABA problem is to encode a \textbf{version number} with any stored data. In this example of the circular buffer, the data would be accompanied by this version number, and the version number would be \textbf{incremented} every time the circular buffer was \textbf{traversed}. With this \textbf{version number}, the thread that was taken off--CPU would have to match \textbf{both the version number and the data} for it to successfully add or remove an element from the buffer.
\par
Adding the version number has \textbf{reduced but not eliminated} the possibility that a thread might return to the CPU to find a match of both version number and data. However, the probability can be made so small as to be practically impossible.
\par
We could modify the circular buffer to include a version number for each element of the buffer, for example, a structure of two integers to hold the value to be stored and the version number. 
\begin{lstlisting}
union ABAvalue
{
	long long llvalue;
	
	struct element
	{
		int version;
		int value;
	} nested;
};

union ABAvalue buffer[16];

int counter = 0;

void clearbuffer()
{
	addhere = 0;
	removehere = 0;
	for ( int i=0; i<16;i++ )
	{
		buffer[i].llvalue = 0;
	}
}


int next( int current )
{
	return ( current + 1 ) & 15;
}

int nextupdate( int current )
{
	if ( current == 15 ) { counter++; }
	return ( current + 1 ) & 15 ;
}
\end{lstlisting}
 The compare and swap operation needs to work on 8-byte values; hence, the assembly for the compare and swap needs to be modified, and the x86 code will work only when compiled to use 64--bit instruction set extensions.
\begin{lstlisting}
#ifdef __sparc
long long CAS( volatile long long* addr, long long ov, long long nv)
{
	asm volatile( 
		"casx %1, %2, %0":
		"=r"(nv):
		"m"(*addr),"r"(ov),"0"(nv):
		"memory" 
	);

	return nv;
}
#else
long long CAS( volatile long long * addr, long long ov, long long nv )
{
	asm volatile( 
		"lock; cmpxchg %2, %1":
		"=a"(ov):
		"m"(*addr),"r"(nv),"a"(ov):
		"memory"
	);
	return ov;
}
#endif

void addtobuffer( int value )
{
	int success = 0;
	union ABAvalue current;
	while ( !success )
	{
		current = buffer[ next(addhere) ];
		if ( current.nested.value == 0 )
		{
			union ABAvalue nextvalue;
			nextvalue.nested.version = counter;
			nextvalue.nested.value = value;
			if ( CAS( &buffer[ next(addhere) ].llvalue,
					current.llvalue,
					nextvalue.llvalue )	== current.llvalue )
			{
				addhere = nextupdate(addhere);
				success = 1;
			}
		}
	}
}

int removefrombuffer()
{
	union ABAvalue current;
	int success = 0;
	int value;
	while ( !success )
	{
		current = buffer[ next(removehere) ];
		if ( current.nested.value != 0 )
		{
			value = current.nested.value;
			union ABAvalue nextvalue;
			nextvalue.nested.version = counter;
			nextvalue.nested.value = 0;
			if ( CAS( &buffer[ next(removehere)].llvalue,
						current.llvalue,
						nextvalue.llvalue )	== current.llvalue )
			{
				removehere = next(removehere);
				success = 1;
			}
		}
	}
	return value;
}
\end{lstlisting}
\section{Priority Inversion}
priority inversion: where one thread is waiting for the resources held by another thread. If the thread holding the resources is of lower priority, then the priority is said to be inverted.
\\
A potential issue in this situation is when a low-priority thread holds a mutex. A thread with a higher priority wakes up and causes the low-priority thread to be descheduled from the CPU. The thread waiting for the mutex could be of an even higher priority, but it will now have to wait for both the lower-priority threads to complete before it can acquire the mutex.
\\
A common solution for this problem is for the thread holding the resources to temporarily acquire the max priority of the waiting threads, ideally reducing the time that it holds the resources. Once the thread releases the resources, it resumes its original lower priority, and the higher-priority thread continues execution with its original higher priority.
\\
An alternative solution is to boost the priority of any thread that acquires a particular resource so that the thread is unlikely to be preempted while it is holding the resource.

\section{False Sharing}
\textbf{False sharing} is the situation where multiple threads are accessing items of data held on a \textbf{single cache line}. Although the threads are all using separate items of data, the cache line itself is shared between them so \textbf{only} a single thread can write to it at any one time. This is purely a performance issue because there is no correctness issue. It would be a correctness issue if it were a single variable on the cache line being shared between the threads.
\par
The performance impact comes from the fact that each thread requires the cache line to be present and writable in the cache of the processor where the thread is executing. If another thread recently wrote to the cache line, then the modified data needs to be written back to memory and then sent to the next processor that wants to write to it. This can cause accesses to the cache line to take a similar length of time as a \textbf{miss} to memory. In the case of false sharing, the line is constantly being bounced between processors, so most accesses to it end up requiring another processor to write the line back to memory--so the line does not ever get the benefit of being cache resident.
\par
The following code demonstrate the cost of false sharing.
\begin{itemize}
	\item Each thread is assigned a \textbf{volatile} variable to use as a counter. The fact that the variable is volatile ensures that the code must store and reload it with every iteration. It also ensures that the compiler cannot eliminate the loop, even though the work it performs is redundant. 
	\item The code creates multiple threads and then times how long it takes the first thread to complete the same amount of work as the other threads.
\end{itemize}
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/time.h>

double now()
{
	struct timeval time;
	gettimeofday( &time, 0 );
	return (double)time.tv_sec + (double)time.tv_usec / 1000000.0;
}

#define COUNT 100000000
volatile int go = 0;
volatile int counters[20];

void *spin( void *id )
{
	int myid = (int)id + 1;
	while( !go ) {}
	counters[myid] = 0;
	while ( counters[myid]++ < COUNT ) {}
}

int main( int argc, char* argv[] )
{
	pthread_t threads[256];
	int nthreads = 1;
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	for( int i=1; i<nthreads; i++ )
	{
		pthread_create( &threads[i], 0, spin, (void*)i );
	}
	double start = now();
	go = 1;
	spin( 0 );
	double end = now();
	printf("Time %f ns\n", ( end ? start ) );
	
	for( int i=0; i<nthreads; i++ )
	{
		pthread_join( threads[i], 0 );
	}
	return 0;
}
\end{lstlisting}
It is very easy to solve false sharing by \textbf{padding} the accessed structures so that the variable used by each thread resides on a \textbf{separate} cache line. The cache line can then reside in the cache of the processor where the thread is running, and consequently, all accesses to that cache line are low cost, and the code runs much faster.
\par
The following gives the modified code where access tho the counter structure have been padded so that each counter is located at \textbf{64--byte} intervals. This will ensure that the variables are located on separate cache lines on machines with cache line sizes of 64 bytes or less.
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/time.h>

double now()
{
	struct timeval time;
	gettimeofday( &time, 0 );
	return (double)time.tv_sec + (double)time.tv_usec / 1000000.0;
}

#define COUNT 100000000
volatile int go = 0;
volatile int counters[320];

void *spin( void *id )
{

	int myid = ( (int)id + 1) * 16;
	while( !go ) {}
	counters[myid] = 0;
	while ( counters[myid]++ < COUNT ) {}
}

int main( int argc, char* argv[] )
{
	pthread_t threads[256];
	int nthreads = 1;
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	nthreads--;
	for( int i=1; i<nthreads+1; i++ )
	{
		pthread_create( &threads[i], 0, spin, (void*)i );
	}
	
	double start = now();
	go=1;
	spin( 0 );
	double end = now();
	printf( "Time %f s\n", ( end ? start ) );
	
	for( int i=0; i<nthreads; i++ )
	{
		pthread_join( threads[i], 0 );
	}
	return 0;
}

\end{lstlisting}
Although fixing false sharing is easy to do in most cases, detecting performance loss from it is much harder.
\begin{itemize}
	\item In general, false sharing will turn up as an elevated number of cache misses on a particular memory operation, and it is hard to distinguish this from the normal cache misses that occur in all applications.
	\item However, the important thing to realize from the previous description is that when significant time is lost to false sharing, there will be significant numbers of cache misses, indicating the points at which the false sharing occurs.
	\item Hence, tracking down false sharing can be as simple as locating the places in the code where there are unexpectedly high numbers of cache misses on variables that should be local to one thread.
\end{itemize}
One important thing to note is that false sharing is a \textbf{significant issue} for \textbf{multiprocessor} systems but is \textbf{not} nearly as critical for \textbf{multicore} systems. The data is shared through the on-chip caches, and this sharing has \textbf{little} impact on performance. This is a useful feature of multicore processors.
\section{Cache Conflict and Capacity}
One of the notable features of multicore processors is that threads will share a single cache at some level. There are two issues that can occur with shared caches: \textbf{capacity misses} and \textbf{conflict misses}.
\subsection{Conflict Miss}
\textbf{A conflict cache mis}s is where one thread has caused data needed by another thread to be evicted from the cache. The worst example of this is \textbf{thrashing} where multiple threads each require an item of data and that item of data maps to the same cache line for all the threads. Shared caches usually have sufficient associativity to avoid this being a significant issue. However, there are certain attributes of computer systems that tend to make this likely to occur.
\par
Data structures such as stacks tend to be aligned on cache line boundaries, which increases the likelihood that structures from different processes will map onto the same address. Consider the code shown as below. This code creates a number of threads. Each thread prints the address of the first item on its stack and then waits at a barrier for all the threads to complete before exiting.
\begin{lstlisting}
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

pthread_barrier_t barrier;

void* threadcode( void* param )
{
	int stack;
	printf("Stack base address = %x for thread %i\n", &stack, (int)param);
	pthread_barrier_wait( &barrier );
}


int main( int argc, char*argv[] )
{
	pthread_t threads[20];
	int nthreads = 8;
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	pthread_barrier_init( &barrier, 0, nthreads );
	
	for( int i=0; i<nthreads; i++ )
	{
		pthread_create( &threads[i], 0, threadcode, (void*)i );
	}
	
	for( int i=0; i<nthreads; i++ )
	{
		pthread_join( threads[i], 0 );
	}
	
	pthread_barrier_destroy( &barrier );
	
	return 0;
}
\end{lstlisting}
The expected output when this code is run on 32-bit Solaris indicates that threads are created with a 1MB offset between the start of each stack. For a processor with a cache size that is a power of two and \textbf{smaller} than 1MB, a stride of 1MB would ensure the base of the stack for all threads is in the \textbf{same set} of cache lines.
\par
The \textbf{associativity} of the cache will \textbf{reduce} the chance that this would be a problem. A cache with an associativity \textbf{greater} than the \textbf{number of threads} sharing is less likely to have a problem with conflict misses.
\par
Suppose an application has multiple threads, and they all execute common code, spending the majority of the time performing calculations in the same routine. 
\begin{itemize}
	\item If that routine performs a lot of stack accesses, there is a good chance that the threads will conflict, causing thrashing and poor application performance.
	\item  This is because all stacks start at some multiple of the stack size. The same variable, under the same call stack, will appear at the same offset from the base of the stack for all threads.
	\item It is quite possible that the cache line will be mapped onto the same cache line set for all threads. If all threads make heavy use of this stack location, it will cause thrashing within the set of cache lines.
	\item It is for this reason that processors usually implement some kind of \textbf{hashing} in hardware, which will cause addresses with a strided access pattern to map onto different sets of cache lines. 
\end{itemize}
\subsection{Capacity Miss}
Capacity misses is the situation where the data set that a single thread uses fits into the cache, but adding a second thread causes the total data footprint to exceed the capacity of the cache.
\par
In the code as below, each thread allocates an \textbf{8KB} array of integers.
\begin{itemize}
	\item When the code is run with a single thread on a core with at least 8KB of cache, the data the thread uses becomes cache resident, and the code runs quickly.
	\item If a second thread is started on the same core, the two threads would require a total 16KB of cache for the data required by both threads to remain resident in cache.
\end{itemize}
\begin{lstlisting}
#include <pthread.h>
#include <stdio.h>
#include <sys/time.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/processor.h>
#include <sys/procset.h>

void * threadcode( void*id )
{
	int *stack = calloc( sizeof(int), 2048 );
	processor_bind( P_LWPID, P_MYID, ((int)id*4) & 63, 0 );
	for( int i=0; i<1000; i++ )
	{
		hrtime_t start = gethrtime();
		double total = 0.0;
		for( int h=0; h<100; h++ )
		for( int k=0; k<256*1024; k++ )
		total += stack[ ( (h*k) ^ 20393 ) & 2047 ]
		*stack[ ( (h*k) ^ 12834 ) & 2047 ];
		hrtime_t end = gethrtime();
		if ( total == 0 ){ printf( "" ); }
		printf( "Time %f ns %i\n", (double)end ? (double)start, (int)id );
	}
}


int main( int argc, char*argv[] )
{
	pthread_t threads[20];
	int nthreads = 8;
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	for( int i=0; i<nthreads; i++ )
	{
		pthread_create( &threads[i], 0, threadcode, (void*)i );
	}
	for( int i=0; i<nthreads; i++) { pthread_join( threads[i], 0 ); }
	return 0;
}
\end{lstlisting}
Running this code on an UltraSPARC T2 processor with one thread reports a time of about 0.7 seconds per iteration of the outermost loop. The 8KB data structure fits into the 8KB cache. When run with two threads, this time nearly doubles to 1.2 seconds per iteration, as might be expected, because the required data exceeds the size of the first-level cache and needs to be fetched from the \textbf{shared second--level cache}.
\par
There are 2 ways to determine whether this was the problem.
\begin{enumerate}
	\item The 1st way is to perform the experiment where the size of the data used by each thread is reduced so that the \textbf{combined footprint} is much less than the size of the \textbf{cache}. If this is done and there is no impact performance from adding a second thread, it indicates that the problem is a cache capacity issue and not because of the two threads sharing instruction issue width.
	\item An alternative way to identify the same issue is to look at \textbf{cache miss rates} using the \textbf{hardware performance counters} available on most processors.
\end{enumerate}
One tool to access the hardware performance counters on Solaris is cputrack. This tool reports the number of hardware performance counter events triggered by a single process. 
\par
It is important to minimize the memory footprint of the codes running on the system. Most \textbf{memory footprint optimizations} will tend to appear to be common good programming practice and are often automatically implemented by the compiler. For example, if there are unused local variables within a function, then the compiler will not allocate stack space to hold them.
\par
Consider an application that manages its own memory. It would be better for this application to return recently deallocated memory to the thread that deallocated it rather than return memory that had not been used in a while. The reason is that the recently deallocated memory might still be cache resident. Reusing the same memory avoids the cache misses that would occur if the memory manager returned an address that had not been recently used and the thread had to fetch the cache line from memory.
\section{Pipeline Resource Starvation}
If a single thread is running on a core, its performance is limited by the rate at which the core can issue instructions and the number of stalls that the thread encounters. For example, a core might be able to sustain only a single load every cycle, so a code that contained only load instructions could at most issue a single instruction per cycle. Similarly, a code that consisted of a stream of dependent load instructions would only be able to issue the next instruction once the previous load had completed, and the time the load would take would represent the latency of the memory system.
\par
When multiple threads share a core, the rate at which the threads can issue instructions is limited by two constraints.
\begin{enumerate}
	\item The 1st constraint is the\textbf{ maximum rate at which the hardware can issue instructions}. For example, if there is only one load pipeline, then only one load instruction can be issued per cycle.
	\item The 2nd constraint is that \textbf{all the threads have to share the available hardware resources}. If there is one load pipeline and two threads, then only one thread can issue a load instruction at a time.
\end{enumerate}
It is instructive to think of this in terms of cache misses. When one thread is stalled waiting for data to be returned from memory, then all the other threads on that core can take instruction issue slots from the stalled thread. If all threads are stalled waiting for data from memory, then the issue width of the processor is not the dominating constraint on performance. This is the ideal situation for a CMT processor. If the threads are stalled waiting for memory, then it is more effective to have a large number of these threads all making forward process. Although individual threads might run slowly, the aggregate throughput of the system is impressive because of the number of threads.
\par
When threads are not stalled on memory, such as when the data is cache resident or the code is compute intensive, the threads can become limited on instruction issue width. A single core supporting multiple threads relies on gaps where each thread is stalled to find cycles where an instruction from a different thread can be executed. If there are too many compute-intensive threads running on the core, then instruction issue width becomes a limiting factor in performance.
\par
Consider the code shown as below, which runs a number of threads, and each thread is performing a simple set of integer operations on an item of data. Integer operations are typically single-cycle instructions, so one thread will desire to execute a single instruction every cycle. This is achievable when there is a single thread running on the core, but it can be limited on some processors when multiple threads are assigned to the same core.
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <strings.h>
#include <pthread.h>

#include <sys/types.h>
#include <sys/processor.h>
#include <sys/procset.h>

int nthreads = 8;

void *experiment( void *id )
{
	unsigned int seed = 0;
	
	processor_bind( P_LWPID, P_MYID, ((int)id*2), 0 );
	int count = 100000000;
	for( int i=0; i<count; i++ )
	{
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
	}
	if ( seed == 1 ) { printf( "" ); }
}

int main( int argc, char* argv[] )
{
	pthread_t threads[64];
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	for( int i=0; i<nthreads; i++ )
	{
		pthread_create( &threads[i], 0, experiment, (void*)i );
	}
	for( int i=0; i<nthreads; i++ )
	{
		pthread_join( threads[i], 0 );
	}
	return 0;
}

\end{lstlisting}
The code uses the Solaris \textbf{processor\_bind} call to \textbf{bind} threads to particular \textbf{virtual} CPUs. This is necessary because the operating system will typically try to locate threads so that each thread gets the maximal resources. In this instance, we do not want that to happen because we want to investigate what happens when threads are scheduled on the same core. Doing this correctly requires knowledge of how the virtual CPU number maps to cores. This mapping often falls into one of two types.
\begin{itemize}
	\item The one common mapping is that \textbf{virtual CPU numbers are assigned to cores in groups}. If a core supports \textbf{four} threads, then virtual CPUs \textbf{0} to \textbf{3} would correspond to the four threads on the the \textbf{first} core.
	\item 	The other common mapping scheme is to \textbf{interleave} cores. In this case, if there were \textbf{two} cores, then all the \textbf{odd}-numbered virtual CPUs would map to one core and all the \textbf{even}-numbered virtual CPUs would map to the other core.
\end{itemize}
The code as shown is for a machine where the virtual CPUs are \textbf{interleaved}. The machine had two hyperthreading--enabled cores, making a total of four virtual CPUs. Virtual CPUs \textbf{0} and \textbf{2} are located on core \textbf{0}, and virtual CPUs \textbf{1} and \textbf{3} are located on core \textbf{1}.
\par
When the bit manipulation code is run with one thread, it takes about two seconds. When run with two threads on the same core, it runs in two and a half seconds. If the runtime had remained the same, then twice as much work would have been achieved in the same time. However, this was not the case---twice as much work was achieved in 25\% more time. To put it another way, this represents a 60\% gain in throughput.
\par
The same code was also run on an UltraSPARC T2 machine, with the binding suitably modified. The topology of this machine is interesting because each core can support eight threads, and these threads are arranged into two groups of four. Each group of four can issue one integer operation per cycle, but the two groups share a single load/store pipeline and a single floating--point pipeline. When run on this machine, using a single thread, the code took four seconds. When two threads were assigned to the same group on the same core, the threads completed in seven seconds, achieving only slightly more work per unit time than a single thread. When the two threads are assigned to different groups on the same core, the threads complete their work in four seconds. With this distribution of threads, the amount of work achieved per unit time is doubled.
\par
As more threads become active, the scaling of a processor depends on the architecture of the core. The more available capacity, the more the throughput of the core will increase as the number of threads increases. An alternative view is that cores that work very hard to extract every bit of performance from a single--threaded code will end up with fewer spare cycles for a second thread to use. Once a core becomes fully utilized, additional threads will steal cycles from those that are already running and will cause all the active threads to run slightly slower.
\par
If all the threads are busy performing useful work, then to a large extent it does not matter how well the processor scales--as long as it is completing more work than it was when running with a single thread.
\par
Where performance is potentially lost is when one of the threads is spinning, waiting for some signal. The spinning thread will be taking instruction issue opportunities from the other threads running on the core. The code in the following demonstrates how a spinning thread can detract from the performance of the worker thread. This code creates multiple spinning threads and then attempts to complete a predefined amount of work.
\begin{lstlisting}
#include <stdio.h>
#include <stdlib.h>
#include <strings.h>
#include <pthread.h>

#include <sys/types.h>
#include <sys/processor.h>
#include <sys/procset.h>

int nthreads = 8;
volatile int busy = 1;

void * spin( void *id )
{
	processor_bind( P_LWPID, P_MYID, (int)id, 0 );
	while ( busy ) {}
}

void experiment( int id )
{
	unsigned int seed = 0;
	processor_bind( P_LWPID, P_MYID, id, 0 );
	int count = 100000000;
	for( int i=0; i<count; i++ )
	{
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
		seed += (seed<<2) ^ (seed|33556);
	}
	if ( seed == 1 ) { printf( "" ); }
}

int main( int argc, char* argv[] )
{
	pthread_t threads[64];
	if ( argc > 1 ) { nthreads = atoi( argv[1] ); }
	for( int i=0; i<nthreads; i++ )
	{
		pthread_create( &threads[i], 0, spin, (void*)(i+1) );
	}
	experiment( 0 );
	busy = 0;
	for( int i=0; i<nthreads; i++ )
	{
		pthread_join( threads[i], 0 );
	}
	return 0;
}
\end{lstlisting}
On a test machine, the code takes \textbf{4} seconds to complete when there are no other threads scheduled to the same core. With \textbf{one spinning thread} scheduled to the same core, the code takes \textbf{5.5} seconds to complete. With \textbf{2} threads it takes \textbf{8} seconds, and with \textbf{3} spinning threads it takes \textbf{12} seconds.
\par
The 1st spinning thread slows the original thread by only about 25\% because the spinning thread mainly performs load and branch instructions, both of which have a latency of a few cycles before the thread is again ready to execute a new instruction. The net result is that the first spinning thread wants to issue an instruction every 4 cycles, which leaves three cycles for the original thread to execute instructions.
\par
The 2nd spinning thread follows the same pattern, so now there are two threads that want to issue an instruction every four cycles. This leaves the original thread able to issue an instruction every other cycle, so it runs twice as slowly.
\par
When the 3rd spinning thread is added, the core is already fully occupied by running the original thread and the two spinning threads, so adding an additional thread takes the situation from one where the \textbf{3} threads are getting to issue about one instruction every three cycles to a situation where they get to issue an instruction every four cycles. This is a 33\% slowdown, which takes the runtime from 8 to 12 seconds.
\par
One way of reducing the impact of spinning threads is to cause the thread to pause and consume no instruction issue resources. Most processors have a long latency instruction that can be used for this purpose even if they do not have an explicit instruction for delaying.
\end{document}          
